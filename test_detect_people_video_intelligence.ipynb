{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This script uses the person detection module\n",
    "# of the Google Cloud Video Intelligence API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import os\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"]=\"Project-4484a7a8a88b.json\"\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from google.cloud import videointelligence_v1 as videointelligence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_person(local_file_path=\"video/DIOR_test_video.mp4\"):\n",
    "    \"\"\"Detects and tracks people in a video from a local file.\"\"\"\n",
    "\n",
    "    client = videointelligence.VideoIntelligenceServiceClient()\n",
    "\n",
    "    with io.open(local_file_path, \"rb\") as f:\n",
    "        input_content = f.read()\n",
    "\n",
    "    # Configure the request\n",
    "    config = videointelligence.types.PersonDetectionConfig(\n",
    "        include_bounding_boxes=True,\n",
    "        include_attributes=False,\n",
    "        include_pose_landmarks=False,\n",
    "        \n",
    "    )\n",
    "    context = videointelligence.types.VideoContext(\n",
    "        person_detection_config=config\n",
    "    )\n",
    "\n",
    "    # Start the asynchronous request\n",
    "    operation = client.annotate_video(\n",
    "        request={\n",
    "            \"features\": [videointelligence.Feature.PERSON_DETECTION],\n",
    "            \"input_content\": input_content,\n",
    "            \"video_context\": context,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    print(\"\\nProcessing video for person detection annotations.\")\n",
    "    result = operation.result(timeout=300)\n",
    "\n",
    "    print(\"\\nFinished processing.\\n\")\n",
    "\n",
    "    # Retrieve the first result, because a single video was processed.\n",
    "    annotation_result = result.annotation_results[0]\n",
    "    \n",
    "    # Create list containing timestamps and boxes with detection\n",
    "    dat_lst = []\n",
    "\n",
    "    # loop through people detected\n",
    "    for annotation in annotation_result.person_detection_annotations:\n",
    "        \n",
    "         for track in annotation.tracks:\n",
    "            \n",
    "            # Get box coordinates for each person in each frame\n",
    "            for timestamped_object in track.timestamped_objects:\n",
    "                box = timestamped_object.normalized_bounding_box\n",
    "#                 print(\"Time is:\")\n",
    "#                 print(\n",
    "#                         \"\\{}s\".format(\n",
    "#                             timestamped_object.time_offset.seconds\n",
    "#                             + timestamped_object.time_offset.microseconds / 1e6,)\n",
    "#                 )            \n",
    "#                 print(\"Bounding box:\")\n",
    "#                 print(\"\\tleft  : {}\".format(box.left))\n",
    "#                 print(\"\\ttop   : {}\".format(box.top))\n",
    "#                 print(\"\\tright : {}\".format(box.right))\n",
    "#                 print(\"\\tbottom: {}\".format(box.bottom))\n",
    "#                 print\n",
    "            \n",
    "                # Append relevant data from the timestamped object\n",
    "                dat_lst.append([\n",
    "                    (track.segment.start_time_offset.seconds\n",
    "                    + track.segment.start_time_offset.microseconds / 1e6),\n",
    "                    (track.segment.end_time_offset.seconds\n",
    "                    + track.segment.end_time_offset.microseconds / 1e6),\n",
    "                    (timestamped_object.time_offset.seconds\n",
    "                    + timestamped_object.time_offset.microseconds / 1e6),\n",
    "                    box.left,\n",
    "                    box.top,\n",
    "                    box.right,\n",
    "                    box.bottom\n",
    "                ])\n",
    "    \n",
    "    columns = ['Begin','End', 'Time','BoxLeft', 'BoxTop', 'BoxRight', 'BoxBottom']\n",
    "    person_dat = pd.DataFrame(dat_lst, columns=columns)\n",
    "    \n",
    "    return person_dat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing video for person detection annotations.\n",
      "\n",
      "Finished processing.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "person_dat = detect_person()\n",
    "# detect_person()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We then use the openCV library to\n",
    "# create a new video displaying the results\n",
    "\n",
    "# Create a VideoCapture object and read from input file\n",
    "cap = cv2.VideoCapture('video/DIOR_test_video.mp4')\n",
    "\n",
    "# Check if camera opened successfully\n",
    "if (cap.isOpened()== False): \n",
    "  print(\"Error opening video stream or file\")\n",
    "\n",
    "# Default resolutions of the frame are obtained.The default resolutions are system dependent.\n",
    "# We convert the resolutions from float to integer.\n",
    "frame_width = int(cap.get(3))\n",
    "frame_height = int(cap.get(4))\n",
    "\n",
    "# Define the codec and create VideoWriter object.\n",
    "fourcc = cv2.VideoWriter_fourcc(*'MP4V') \n",
    "out = cv2.VideoWriter('results/DIOR_test_result_video_intelligence.mp4', fourcc, 25., (frame_width,frame_height))\n",
    "\n",
    "# Adapt the box coordinates to the frame width and height\n",
    "box_dat = person_dat.copy()\n",
    "box_dat['BoxLeft'] = round(box_dat.BoxLeft * frame_width)\n",
    "box_dat['BoxRight'] = round(box_dat.BoxRight * frame_width)\n",
    "box_dat['BoxTop'] = round(box_dat.BoxTop * frame_height)\n",
    "box_dat['BoxBottom'] = round(box_dat.BoxBottom * frame_height)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw boxes around people detected frame by frame\n",
    "i_frame = 0\n",
    "while(True):\n",
    "    ret, frame = cap.read()\n",
    "    \n",
    "    if ret == True:\n",
    "    \n",
    "        timestamp = i_frame / 24\n",
    "        i_frame = i_frame + 1\n",
    "        \n",
    "        # Draw a box around people detected in the frame \n",
    "        if box_dat['Time'].sub(timestamp).abs().min() <= 0.1:\n",
    "            \n",
    "            # Check the number of people detected in the frame\n",
    "            result_index = box_dat['Time'].sub(timestamp).abs().idxmin()\n",
    "            \n",
    "            box_dat_detec = box_dat.loc[box_dat['Time'] == box_dat['Time'].iloc[result_index]]\n",
    "            box_dat_detec.apply(\n",
    "            lambda row: cv2.rectangle(\n",
    "                frame, \n",
    "                (int(row['BoxLeft']),int(row['BoxTop'])), \n",
    "                (int(row['BoxRight']),int(row['BoxBottom'])), \n",
    "                (0,255,0), 5),\n",
    "                axis=1\n",
    "            )\n",
    "    \n",
    "        cv2.putText(frame, 'TEST RESULT', (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 255), 2)\n",
    "        #cv2.putText(frame, str(i_frame), (150, 150), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 255), 2)\n",
    "        \n",
    "        # Write the frame into the file 'output.avi'\n",
    "        out.write(frame)\n",
    "\n",
    "\n",
    "#     # Press Q on keyboard to stop recording\n",
    "#         if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "#             break\n",
    "\n",
    "  # Break the loop\n",
    "    else:\n",
    "        break  \n",
    "\n",
    "# When everything done, release the video capture and video write objects\n",
    "cap.release()\n",
    "out.release()\n",
    "\n",
    "# Closes all the frames\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
